# ADR-013 — Test Strategy and Golden Fixture Design

**Status:** Accepted
**Date:** 2026-02-20

---

## Context

This project requires deterministic outputs across multiple inputs and generators:

* Script-based comment help → MAML + HelpInfo.xml + Markdown
* Compiled cmdlets → MAML + HelpInfo.xml + Markdown
* Artifact ingestion (MAML/HelpInfo.xml) → Markdown

To prevent regressions and ensure stability, tests must use golden fixtures and directory-tree comparisons.

This ADR defines the test strategy and fixture layout.

---

## Decision

### 1. Test layers

We implement three test tiers:

1. **Unit Tests**

   * Parsing of directives into normalized model
   * Reflection extraction from compiled cmdlets
   * Markdown rendering for a single command
   * MAML node generation rules (ordering, presence)

2. **Integration Tests**

   * End-to-end for a sample script module
   * End-to-end for a sample compiled module
   * Validate generated artifacts against schemas
   * Validate directory layout contracts

3. **Golden Fixture Tests**

   * Generate outputs into a temp folder
   * Compare folder trees + file contents to a committed “golden” baseline

---

### 2. Fixture repository layout

```text
/tests
  /fixtures
    /script-module-basic
      input/
        MyModule.psm1
        MyModule.psd1
      expected/
        help/
        docs/
        updatable-help/
    /script-module-missing-help
      input/
      expected/
    /compiled-module-basic
      input/
        MyModule.dll
        MyModule.xml
        MyModule.psd1 (optional)
      expected/
        help/
        docs/
        updatable-help/
    /maml-ingest-basic
      input/
        help/en-US/MyModule-Help.xml
        HelpInfo.xml
      expected/
        docs/
```

Rules:

* `expected/` is the golden output.
* Tests must run generation and compare outputs exactly.

---

### 3. Golden comparison rules (deterministic diffing)

Golden comparisons must include:

* Directory structure equality
* File presence equality
* Exact file contents equality

Normalization for comparison:

* Line endings normalized to LF before compare
* Trailing whitespace not ignored (must be deterministic)
* XML normalized only if generator claims canonical formatting; otherwise compare raw text

When diff fails:

* Test output prints:

  * Missing files
  * Extra files
  * First differing line for file content mismatch

---

### 4. Schema validation tests

We validate generated artifacts:

* HelpInfo.xml:

  * XSD validation where feasible
  * Semantic rules per ADR-009 and ADR-010

* MAML:

  * Well-formed XML validation always
  * XSD validation where feasible
  * Semantic rules per ADR-007/ADR-009

If XSD availability is offline-cached:

* Tests must include the schema bundle in the repo or in test assets.

---

### 5. Cross-source parity tests

We include parity tests to prove that script and compiled sources generate equivalent structures for the same conceptual command:

* Same command name
* Same parameter list and parameter metadata
* Same section ordering in markdown
* Equivalent MAML structure

Prose differences are allowed if authored differently.

---

### 6. Performance guardrails

We add basic performance regression tests (non-benchmark) to ensure:

* Parsing does not scale superlinearly with file size
* Reflection does not load assemblies repeatedly unnecessarily

These are informational unless thresholds are exceeded in CI.

---

### 7. “Update golden fixtures” workflow

We intentionally make golden updates explicit:

* Tests do not auto-update expected files.
* A dedicated dev command/script generates outputs into `tests/fixtures/**/expected`.
* Fixture update requires a conscious commit + review.

---

## Consequences

Positive:

* Strong regression protection
* Deterministic outputs enforced by tests
* Confident refactoring and generator iteration

Trade-offs:

* Golden fixtures require maintenance when contracts evolve
* Early development may see frequent fixture updates until stabilized
